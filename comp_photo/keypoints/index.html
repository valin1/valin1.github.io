<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Modeled off of fimbo.-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title> Facial Keypoint Detection </title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css">
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.1/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
    <style type="text/css" media="screen">
@import url('https://fonts.googleapis.com/css?family=Nanum+Gothic');

body {
    font-family: 'Nanum Gothic', sans-serif;
    padding:0px;
    margin:0px;
    max-width:100vw;
    overflow-x:hidden;
    scroll-behavior: smooth;
}

a {
    color: #5b93b0;
    -webkit-transition: all 0.3s ease-in-out;
    transition: all 0.3s ease-in-out;
    text-decoration:none;
}

a:hover, a:focus, a:active {
    color: #375d70;
}

a:hover, a:active {
    outline: 0;
}

h1, h2, h3 {
    color: #4d4f52;
}

h4 {
    color: #4d4f52;
    line-height: 1.5;
}

header {
    width:92vw;
    padding:6vh 4vw;
    position:fixed;
    background:transparent;
    color:#fff;
    transition:0.4s ease-in-out;
    z-index:2;
}
header table {
    width:100%;
}

strong {
    color:#1179ad;
    font-weight: bold;
    font-size: 18.5px;
}

#photoprojects {
    width:92vw;
    padding:6vh 4vw;
    font-weight:bold;
}
#photoprojects h1 {
    padding:5px 0px;
    width:100%;
}

#footer {
    width:100%;
    padding:10vh 0px;
    text-align:center;
}
#footer a {
    color:#375d70;
    text-decoration:none;
}
::placeholder {
    color:#000;
}
button:focus {
    outline:none;
}
::-webkit-scrollbar {
    width:5px;
    height:5px;
}
::-webkit-scrollbar-track {
    background: #f1f1f1; 
}
::-webkit-scrollbar-thumb {
    background: rgb(0, 0, 0); 
}
::-webkit-scrollbar-thumb:hover {
    background: rgba(0, 0, 0,0.8); 
}

#container {
    text-align: center;
}
figure {
    display: inline-block;
}
figcaption {
    margin: 10px 0 0 0;
    font-family: 'Nanum Gothic', sans-serif;
    color: #4d4f52;
}

img:hover {
    transform: scale(1.1);
    -ms-transform: scale(1.1);
    -webkit-transform: scale(1.1);
    -moz-transform: scale(1.1);
    -o-transform: scale(1.1);
}
img {
    transition: transform 0.2s;
    -webkit-transition: -webkit-transform 0.2s;
    -moz-transition: -moz-transform 0.2s;
    -o-transition: -o-transform 0.2s;
}
code {
    padding: .2rem .4rem;
    font-size: 90%;
    color: #2a6e9c;
    background-color: #f8f9fa;
    border-radius: .25rem;
}

.row {
  display: flex;
}

.column {
  flex: 50%;
  padding: 5px;
  text-align: center;
}

.column_3 {
  flex: 33.33%;
  padding: 5px;
  text-align: center;
}

.column_4 {
  flex: 25%;
  padding: 5px;
  text-align: center;
}

.column_5 {
  flex: 20%;
  padding: 5px;
  text-align: center;
}


    </style>
</head>
<body>
    <div id="photoprojects">
        <h1 style= "text-align: center">Facial Keypoint Detection with Neural Networks </h1>
        <h3 style= "text-align: center">Vanessa Lin</h3>

        <h2>Overview</h2>
        In this project, I built convolutional neural networks to help me detect keypoints on a face, from just the nose to the whole face structure (eyes, eyebrows, nose, lips, face outline). To detect these facial keypoints, I first built two toy CNN models to train on the Danish computer scientists dataset. Afterwards, I moved onto a larger dataset, the ibug face dataset, to train and detect facial keypoints. 

        <h2>Nose Tip Detection</h2>
        In this part of the project, I built a simple convolutional neural network that takes in an image as input and outputs the coordinates of a nose tip. Before training, I created a Dataset that creates a sample of <code>{'image': image, 'landmarks': landmarks}</code>, where the image was resized to a smaller size of 80 by 60 and the landmarks would just be one point for the nose. Next, I created a DataLoader to iterate through the data during training and validation. Below are some images from the training set with the ground truth labels in red.
        <div class="row">
          <div class="column_4">
            <br>
            <img src="images/nose_point_0.png" width ="100%"/>
          </div>
          <div class="column_4">
            <br>
            <img src="images/nose_point_1.png" width ="100%"/>
          </div>
          <div class="column_4">
            <br>
            <img src="images/nose_point_2.png" width ="100%"/>
          </div>
          <div class="column_4">
            <br>
            <img src="images/nose_point_3.png" width ="100%"/>
          </div>
        </div>
        For the neural network, I used 3 convolutional layers and 2 fully connected layers:
        <br>
        <br>  
        <li> a convolutional layer with 5x5 filters and 15 output channels followed by a relu and maxpool2d of 2x2</li>
        <li> a convolutional layer with 5x5 filters and 20 output channels followed by a relu and maxpool2d of 2x2</li>
        <li> a convolutional layer with 5x5 filters and 25 output channels followed by a relu and maxpool2d of 2x2</li>
        <li> a fully connected layer with input size of 600 and output size of 200 followed by a relu</li>
        <li> a fully connected layer with input size of 200 and output size of 2</li>
        <br>
        <br>
        I trained the neural network for 25 epochs and used the MSE loss function with an Adam optimizer, using a learning rate of 0.001 and a batch size of 8. Below are the results of the training and validation losses across the 25 epochs.

       <div id="container">
            <figure>
            <img src="images/losses_01.png" width="100%"/>
            <figcaption>Training Process of Nose Keypoint Detection</figcaption>
            </figure>
        </div>

        Here are a few good examples, where the neural network was able to match exactly or near/overlapping the ground truth label. The predicted point is labeled in red, while the ground truth label is labeled in green. (Note: It may be hard to see depending on how large your screen. If you zoom in on webpage to enlargen, it will be easier to see the points.)
        <div class="row">
          <div class="column">
            <br>
            <img src="images/good_nose_0.png" width ="70%"/>
          </div>
          <div class="column">
            <br>
            <img src="images/good_nose_1.png" width ="70%"/>
          </div>
        </div>
        <div class="row">
          <div class="column_4">
            <br>
            <img src="images/good_nose_2.png" width ="70%"/>
          </div>
          <div class="column_4">
            <br>
            <img src="images/good_nose_3.png" width ="70%"/>
          </div>
        </div>

        Here are a few bad examples, where the neural network was completely off on the predicted nose keypoint. As you can see in these images, the faces are either turned away from the camera by a large amount or tilted more towards one direction, which makes it hard to detect the nose point accurately, because the nose is in a different spot due to the turn and tilt. Also, since the nose point is typically located in the lighter area right below the tip of the nose, the different lightings of the images at the tip of the nose may have also caused the neural network to incorrectly predict.
        <div class="row">
          <div class="column_3">
            <br>
            <img src="images/bad_nose_0.png" width ="80%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/bad_nose_1.png" width ="80%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/bad_nose_2.png" width ="80%"/>
          </div>
        </div>


        <h2>Full Facial Keypoints Detection</h2>

        For the full facial keypoints detection, I built a similar convolutional neural network like the nose point detection neural network, but with two more convolutional layers. Also, since we have a small dataset, I added data augmentation, like rotating the image randomly with either <code>[-10, -5, 0, 5, 10]</code> degrees and shifting the image horizontally with either <code>[-10, -5, 0, 5, 10]</code> pixels, and updated the keypoints respectively to each transformation. The images were also resized to 160 by 120. Below are some of the sampled images created with the data augmentation and the ground truth labels in red.
        <div class="row">
          <div class="column">
            <br>
            <img src="images/face_point_0.png" width ="60%"/>
          </div>
          <div class="column">
            <br>
            <img src="images/face_point_1.png" width ="60%"/>
          </div>
        </div>
        <div class="row">
          <div class="column_3">
            <br>
            <img src="images/face_point_2.png" width ="90%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/face_point_3.png" width ="90%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/face_point_4.png" width ="90%"/>
          </div>
        </div>

        For the neural network, I used 5 convolutional layers and 2 fully connected layers:
        <br>
        <br>  
        <li> a convolutional layer with 7x7 filters and 15 output channels followed by a relu</li>
        <li> a convolutional layer with 5x5 filters and 30 output channels followed by a relu</li>
        <li> a convolutional layer with 3x3 filters and 25 output channels followed by a relu and maxpool2d of 2x2</li>
        <li> a convolutional layer with 7x7 filters and 20 output channels followed by a relu</li>
        <li> a convolutional layer with 5x5 filters and 15 output channels followed by a relu and maxpool2d of 2x2</li>
        <li> a fully connected layer with input size of 10560 and output size of 5280 followed by a relu</li>
        <li> a fully connected layer with input size of 5280 and output size of 116 (for the 58 facial points)</li>
        <br>
        <br>   
        I trained the neural network for 20 epochs and used the MSE loss function with an Adam optimizer, using a learning rate of 0.001 and batch size of 4. Below are the results of the training and validation losses across the 20 epochs.      
        <div id="container">
            <figure>
            <img src="images/losses_02.png" width="100%"/>
            <figcaption>Training Process of Facial Keypoints Detection</figcaption>
            </figure>
        </div>

        Here are a few good examples, where the neural network worked pretty well in detecting the facial keypoints although not exactly the same keypoints as the ground truth labels. The predicted point is labeled in red, while the ground truth label is labeled in green.
        <div class="row">
          <div class="column">
            <br>
            <img src="images/face_good01.png" width ="60%"/>
          </div>
          <div class="column">
            <br>
            <img src="images/face_good02.png" width ="60%"/>
          </div>
        </div>
        <div class="row">
          <div class="column_3">
            <br>
            <img src="images/face_good03.png" width ="95%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/face_good04.png" width ="95%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/face_good05.png" width ="95%"/>
          </div>
        </div>

        Here are a few bad examples, where the neural network failed to mark the keypoints at the right places, and created an outline that did not seem to outline any of the facial features. These images, especially the first and third one, probably failed due to how far right they are looking to and not facing the camera directly, since it's hard to distinguish between the nose keypoints and the general chin outline points in those images. Also, the middle photo failed in matching the eyebrows as the neural network probably perceived the shadows of his eyelids as his eyebrows instead because the man was raising his eyebrows higher than the rest of the photos.

        <div class="row">
          <div class="column_3">
            <br>
            <img src="images/face_bad01.png" width ="95%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/face_bad02.png" width ="95%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/face_bad03.png" width ="95%"/>
          </div>
        </div>

        Here are some learned 7x7 filters from the first convolutional layer. 

        <div class="row">
          <div class="column_5">
            <br>
            <img src="images/conv1_1.png" width ="100%"/>
          </div>
          <div class="column_5">
            <br>
            <img src="images/conv1_2.png" width ="100%"/>
          </div>
          <div class="column_5">
            <br>
            <img src="images/conv1_3.png" width ="100%"/>
          </div>
          <div class="column_5">
            <br>
            <img src="images/conv1_4.png" width ="100%"/>
          </div>
          <div class="column_5">
            <br>
            <img src="images/conv1_5.png" width ="100%"/>
          </div>
        </div>

        <div class="row">
          <div class="column_5">
            <br>
            <img src="images/conv1_6.png" width ="100%"/>
          </div>
          <div class="column_5">
            <br>
            <img src="images/conv1_7.png" width ="100%"/>
          </div>
          <div class="column_5">
            <br>
            <img src="images/conv1_8.png" width ="100%"/>
          </div>
          <div class="column_5">
            <br>
            <img src="images/conv1_9.png" width ="100%"/>
          </div>
          <div class="column_5">
            <br>
            <img src="images/conv1_10.png" width ="100%"/>
          </div>
        </div>
        <h2>Train With Larger Dataset</h2>

        Now for training on a larger dataset, like the ibug face in the wild dataset (6666 images and 68 facial points per image), I used the same data augmentation techniques from part (2) for this part during training, and I added another transform that randomly changed the brightness of the photo, which would rescale the intensity through <code>skimage.exposure.rescale_intensity</code> if the rescaling brightness boolean was chosen. As the dataset came with bounding boxes for each image, I increased the bounding box by a factor of 1.25x, cropped each image to the bounding box, and resized the image to 224 by 224 for training. While resizing the images, I also updated the keypoint labels accordingly. For the convolutional neural network, I used ResNet 18 from <code>torchvision.models.resnet18(pretrained=False)</code>. I modified the first convolutional layer to take in 1 channel for the input as the image input's shape was <code>(1, 224, 224)</code>, and I modified the last fully connected layer to an output size of 136, for the 68 facial points. 

        <div class="row">
          <div class="column">
            <br>
            <img src="images/augm_truth1.png" width ="60%"/>
          </div>
          <div class="column">
            <br>
            <img src="images/augm_truth2.png" width ="60%"/>
          </div>
        </div>
        <div class="row">
          <div class="column_3">
            <br>
            <img src="images/augm_truth3.png" width ="95%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/augm_truth4.png" width ="95%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/augm_truth5.png" width ="95%"/>
          </div>
        </div>

        Here is the detailed architecture of the model:
        <br>
        <code>
ResNet( <br>
  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
  (relu): ReLU(inplace=True) <br>
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) <br>
  (layer1): Sequential( <br>
    (0): BasicBlock( <br>
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
      (relu): ReLU(inplace=True) <br>
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
    ) <br>
    (1): BasicBlock( <br>
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
      (relu): ReLU(inplace=True) <br>
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
    ) <br>
  ) <br>
  (layer2): Sequential( <br>
    (0): BasicBlock( <br>
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) <br>
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
      (relu): ReLU(inplace=True) <br>
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
      (downsample): Sequential( <br>
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) <br>
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
      ) <br>
    ) <br>
    (1): BasicBlock( <br>
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
      (relu): ReLU(inplace=True) <br>
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
    )<br>
  ) <br>
  (layer3): Sequential( <br>
    (0): BasicBlock( <br>
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) <br>
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
      (relu): ReLU(inplace=True) <br>
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
      (downsample): Sequential( <br>
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) <br>
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
      ) <br>
    ) <br>
    (1): BasicBlock( <br>
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
      (relu): ReLU(inplace=True) <br>
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
    ) <br>
  ) <br>
  (layer4): Sequential( <br>
    (0): BasicBlock( <br>
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) <br>
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
      (relu): ReLU(inplace=True) <br>
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
      (downsample): Sequential( <br>
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) <br>
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
      ) <br>
    ) <br>
    (1): BasicBlock( <br>
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br> 
      (relu): ReLU(inplace=True) <br>
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) <br>
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) <br>
    ) <br>
  ) <br>
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) <br>
  (fc): Linear(in_features=512, out_features=136, bias=True) <br>
)
        </code>
        <br>
        <br>
        Again, I trained the model with an MSE loss function and an Adam optimizer, using a learning rate of 0.001 and a batch size of 12. Below are the results of the training and validation losses across the 20 epochs. 
        <br>
        <br>
        On Kaggle, the mean absolute error (MAE) of my model is <code><strong>9.73940</strong></code>.

        <div id="container">
            <figure>
            <img src="images/losses_03.png" width="100%"/>
            <figcaption>Training Process of Large Dataset Facial Keypoints Detection</figcaption>
            </figure>
        </div>

        Here are some images from the testing set with the predicted facial keypoints detection.
        <div class="row">
          <div class="column_3">
            <br>
            <img src="images/large_test1.png" width ="100%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/large_test2.png" width ="100%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/large_test3.png" width ="100%"/>
          </div>
        </div> 

        <div class="row">
          <div class="column_3">
            <br>
            <img src="images/large_test4.png" width ="100%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/large_test5.png" width ="100%"/>
          </div>
          <div class="column_3">
            <br>
            <img src="images/large_test6.png" width ="100%"/>
          </div>
        </div> 

        Here are some images from my collection that I ran the model on. Surprisingly, the model did not work as well on the photos of the Asian actors and actresses, like Luo Yunxi, Bai Lu, Park Seo Joon, and Kim Dami; however, it worked on John Krasinski the best and reasonably well for Steve Carell and Jenna Fischer (go The Office cast!) I think one possible reason why the model failed on some of the photos is that for Bai Lu and Kim Dami, their noses are tilted a bit upwards so the tip of the nose is higher then what the usual nose tips of the training set are. Also, the model completely failed at determining the keypoints for Luo Yunxi's photo because he was tilted too far from the camera giving mostly a side profile, which is hard for the model to detect, and the model attempts to place facial points on his cheek.

        <div class="row">
          <div class="column_4">
            <br>
            <img src="images/my_image0.png" width ="100%"/>
            <figcaption>Bai Lu</figcaption>
          </div>
          <div class="column_4">
            <br>
            <img src="images/my_image1.png" width ="100%"/>
            <figcaption>Kim Dami</figcaption>
          </div>
          <div class="column_4">
            <br>
            <img src="images/my_image2.png" width ="100%"/>
            <figcaption>Park Seo Joon</figcaption>
          </div>
          <div class="column_4">
            <br>
            <img src="images/my_image3.png" width ="100%"/>
            <figcaption>Steve Carell</figcaption>
          </div>
        </div>   

        <div class="row">
          <div class="column_4">
            <br>
            <img src="images/my_image4.png" width ="100%"/>
            <figcaption>Jenna Fischer</figcaption>
          </div>
          <div class="column_4">
            <br>
            <img src="images/my_image5.png" width ="100%"/>
            <figcaption>John Krasinski</figcaption>
          </div>
          <div class="column_4">
            <br>
            <img src="images/my_image6.png" width ="100%"/>
            <figcaption>Park Seo Joon</figcaption>
          </div>
          <div class="column_4">
            <br>
            <img src="images/my_image7.png" width ="100%"/>
            <figcaption>Luo Yunxi</figcaption>
          </div>
        </div>   

        <h2>Final Thoughts</h2>
        This project was a lot more interesting than I expected, because I was not looking forward to building the Dataloaders, Datasets, and waiting for the models to run (that part is still not fun); however, it was really cool seeing the results of my model work on some images and be able to pinpoint the specific facial features correctly. The last model would have been pretty useful for the last project in morphing everyone's images, but I don't think I would want to train that long before creating my morphing sequence. Either way, no pain no gain. Also, before I only knew how to use TensorFlow, so it was quite interesting using PyTorch instead.

    </div>
    <div id="footer">
        vanessa lin &copy 2020 <br> 
    </div>
</body>
</html>